{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_9892\\3741198026.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_9892\\3741198026.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([[-10, 0, 10])\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "\n",
    "assert np.isclose(probs[0], 1.0)\n",
    "print(linear_classifer.softmax(np.array([1.0001, 0, 0])),linear_classifer.softmax(np.array([0.9999, 0, 0])) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_14028\\2436061784.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_14028\\2436061784.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_14028\\2436061784.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "#Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "#Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_908\\1806627718.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_908\\1806627718.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "C:\\Users\\Евгений\\AppData\\Local\\Temp\\ipykernel_908\\1806627718.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of linear_classifer failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Евгений\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Евгений\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Евгений\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Евгений\\dlcourse_ai\\assignments\\assignment1\\linear_classifer.py\", line 207\n",
      "    y_bull = y_classes_numers[y_classes_numers = np.max(y_classes_numers,axis=1)]\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "SyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3073, 10)\n",
      "Epoch 0, loss: 78.178582\n",
      "Epoch 1, loss: 70.510608\n",
      "Epoch 2, loss: 69.284558\n",
      "Epoch 3, loss: 69.088457\n",
      "Epoch 4, loss: 69.057089\n",
      "Epoch 5, loss: 69.052071\n",
      "Epoch 6, loss: 69.051269\n",
      "Epoch 7, loss: 69.051140\n",
      "Epoch 8, loss: 69.051120\n",
      "Epoch 9, loss: 69.051116\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23097a5fe80>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.123\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 69.051116\n",
      "Epoch 1, loss: 69.051116\n",
      "Epoch 2, loss: 69.051116\n",
      "Epoch 3, loss: 69.051116\n",
      "Epoch 4, loss: 69.051116\n",
      "Epoch 5, loss: 69.051116\n",
      "Epoch 6, loss: 69.051116\n",
      "Epoch 7, loss: 69.051116\n",
      "Epoch 8, loss: 69.051116\n",
      "Epoch 9, loss: 69.051116\n",
      "Epoch 10, loss: 69.051116\n",
      "Epoch 11, loss: 69.051116\n",
      "Epoch 12, loss: 69.051116\n",
      "Epoch 13, loss: 69.051116\n",
      "Epoch 14, loss: 69.051116\n",
      "Epoch 15, loss: 69.051116\n",
      "Epoch 16, loss: 69.051116\n",
      "Epoch 17, loss: 69.051116\n",
      "Epoch 18, loss: 69.051116\n",
      "Epoch 19, loss: 69.051116\n",
      "Epoch 20, loss: 69.051116\n",
      "Epoch 21, loss: 69.051116\n",
      "Epoch 22, loss: 69.051116\n",
      "Epoch 23, loss: 69.051116\n",
      "Epoch 24, loss: 69.051116\n",
      "Epoch 25, loss: 69.051116\n",
      "Epoch 26, loss: 69.051116\n",
      "Epoch 27, loss: 69.051116\n",
      "Epoch 28, loss: 69.051116\n",
      "Epoch 29, loss: 69.051116\n",
      "Epoch 30, loss: 69.051116\n",
      "Epoch 31, loss: 69.051116\n",
      "Epoch 32, loss: 69.051116\n",
      "Epoch 33, loss: 69.051116\n",
      "Epoch 34, loss: 69.051116\n",
      "Epoch 35, loss: 69.051116\n",
      "Epoch 36, loss: 69.051116\n",
      "Epoch 37, loss: 69.051116\n",
      "Epoch 38, loss: 69.051116\n",
      "Epoch 39, loss: 69.051116\n",
      "Epoch 40, loss: 69.051116\n",
      "Epoch 41, loss: 69.051116\n",
      "Epoch 42, loss: 69.051116\n",
      "Epoch 43, loss: 69.051116\n",
      "Epoch 44, loss: 69.051116\n",
      "Epoch 45, loss: 69.051116\n",
      "Epoch 46, loss: 69.051116\n",
      "Epoch 47, loss: 69.051116\n",
      "Epoch 48, loss: 69.051116\n",
      "Epoch 49, loss: 69.051116\n",
      "Epoch 50, loss: 69.051116\n",
      "Epoch 51, loss: 69.051116\n",
      "Epoch 52, loss: 69.051116\n",
      "Epoch 53, loss: 69.051116\n",
      "Epoch 54, loss: 69.051116\n",
      "Epoch 55, loss: 69.051116\n",
      "Epoch 56, loss: 69.051116\n",
      "Epoch 57, loss: 69.051116\n",
      "Epoch 58, loss: 69.051116\n",
      "Epoch 59, loss: 69.051116\n",
      "Epoch 60, loss: 69.051116\n",
      "Epoch 61, loss: 69.051116\n",
      "Epoch 62, loss: 69.051116\n",
      "Epoch 63, loss: 69.051116\n",
      "Epoch 64, loss: 69.051116\n",
      "Epoch 65, loss: 69.051116\n",
      "Epoch 66, loss: 69.051116\n",
      "Epoch 67, loss: 69.051116\n",
      "Epoch 68, loss: 69.051116\n",
      "Epoch 69, loss: 69.051116\n",
      "Epoch 70, loss: 69.051116\n",
      "Epoch 71, loss: 69.051116\n",
      "Epoch 72, loss: 69.051116\n",
      "Epoch 73, loss: 69.051116\n",
      "Epoch 74, loss: 69.051116\n",
      "Epoch 75, loss: 69.051116\n",
      "Epoch 76, loss: 69.051116\n",
      "Epoch 77, loss: 69.051116\n",
      "Epoch 78, loss: 69.051116\n",
      "Epoch 79, loss: 69.051116\n",
      "Epoch 80, loss: 69.051116\n",
      "Epoch 81, loss: 69.051116\n",
      "Epoch 82, loss: 69.051116\n",
      "Epoch 83, loss: 69.051116\n",
      "Epoch 84, loss: 69.051116\n",
      "Epoch 85, loss: 69.051116\n",
      "Epoch 86, loss: 69.051116\n",
      "Epoch 87, loss: 69.051116\n",
      "Epoch 88, loss: 69.051116\n",
      "Epoch 89, loss: 69.051116\n",
      "Epoch 90, loss: 69.051116\n",
      "Epoch 91, loss: 69.051116\n",
      "Epoch 92, loss: 69.051116\n",
      "Epoch 93, loss: 69.051116\n",
      "Epoch 94, loss: 69.051116\n",
      "Epoch 95, loss: 69.051116\n",
      "Epoch 96, loss: 69.051116\n",
      "Epoch 97, loss: 69.051116\n",
      "Epoch 98, loss: 69.051116\n",
      "Epoch 99, loss: 69.051116\n",
      "Accuracy after training for 100 epochs:  0.123\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3073, 10)\n",
      "Epoch 0, loss: 55.259173\n",
      "Epoch 1, loss: 55.238530\n",
      "Epoch 2, loss: 55.218905\n",
      "Epoch 3, loss: 55.200015\n",
      "Epoch 4, loss: 55.181667\n",
      "Epoch 5, loss: 55.163729\n",
      "Epoch 6, loss: 55.146106\n",
      "Epoch 7, loss: 55.128735\n",
      "Epoch 8, loss: 55.111572\n",
      "Epoch 9, loss: 55.094585\n",
      "Epoch 10, loss: 55.077753\n",
      "Epoch 11, loss: 55.061060\n",
      "Epoch 12, loss: 55.044496\n",
      "Epoch 13, loss: 55.028051\n",
      "Epoch 14, loss: 55.011721\n",
      "Epoch 15, loss: 54.995501\n",
      "Epoch 16, loss: 54.979387\n",
      "Epoch 17, loss: 54.963377\n",
      "Epoch 18, loss: 54.947468\n",
      "Epoch 19, loss: 54.931659\n",
      "Epoch 20, loss: 54.915948\n",
      "Epoch 21, loss: 54.900333\n",
      "Epoch 22, loss: 54.884813\n",
      "Epoch 23, loss: 54.869387\n",
      "Epoch 24, loss: 54.854053\n",
      "Epoch 25, loss: 54.838811\n",
      "Epoch 26, loss: 54.823659\n",
      "Epoch 27, loss: 54.808597\n",
      "Epoch 28, loss: 54.793622\n",
      "Epoch 29, loss: 54.778735\n",
      "Epoch 30, loss: 54.763935\n",
      "Epoch 31, loss: 54.749219\n",
      "Epoch 32, loss: 54.734588\n",
      "Epoch 33, loss: 54.720041\n",
      "Epoch 34, loss: 54.705576\n",
      "Epoch 35, loss: 54.691193\n",
      "Epoch 36, loss: 54.676891\n",
      "Epoch 37, loss: 54.662669\n",
      "Epoch 38, loss: 54.648526\n",
      "Epoch 39, loss: 54.634462\n",
      "Epoch 40, loss: 54.620475\n",
      "Epoch 41, loss: 54.606566\n",
      "Epoch 42, loss: 54.592732\n",
      "Epoch 43, loss: 54.578975\n",
      "Epoch 44, loss: 54.565291\n",
      "Epoch 45, loss: 54.551682\n",
      "Epoch 46, loss: 54.538146\n",
      "Epoch 47, loss: 54.524683\n",
      "Epoch 48, loss: 54.511292\n",
      "Epoch 49, loss: 54.497972\n",
      "Epoch 50, loss: 54.484723\n",
      "Epoch 51, loss: 54.471544\n",
      "Epoch 52, loss: 54.458434\n",
      "Epoch 53, loss: 54.445393\n",
      "Epoch 54, loss: 54.432420\n",
      "Epoch 55, loss: 54.419515\n",
      "Epoch 56, loss: 54.406677\n",
      "Epoch 57, loss: 54.393906\n",
      "Epoch 58, loss: 54.381200\n",
      "Epoch 59, loss: 54.368559\n",
      "Epoch 60, loss: 54.355984\n",
      "Epoch 61, loss: 54.343472\n",
      "Epoch 62, loss: 54.331024\n",
      "Epoch 63, loss: 54.318640\n",
      "Epoch 64, loss: 54.306318\n",
      "Epoch 65, loss: 54.294059\n",
      "Epoch 66, loss: 54.281861\n",
      "Epoch 67, loss: 54.269724\n",
      "Epoch 68, loss: 54.257649\n",
      "Epoch 69, loss: 54.245633\n",
      "Epoch 70, loss: 54.233677\n",
      "Epoch 71, loss: 54.221781\n",
      "Epoch 72, loss: 54.209944\n",
      "Epoch 73, loss: 54.198165\n",
      "Epoch 74, loss: 54.186444\n",
      "Epoch 75, loss: 54.174781\n",
      "Epoch 76, loss: 54.163175\n",
      "Epoch 77, loss: 54.151625\n",
      "Epoch 78, loss: 54.140132\n",
      "Epoch 79, loss: 54.128695\n",
      "Epoch 80, loss: 54.117314\n",
      "Epoch 81, loss: 54.105988\n",
      "Epoch 82, loss: 54.094717\n",
      "Epoch 83, loss: 54.083500\n",
      "Epoch 84, loss: 54.072337\n",
      "Epoch 85, loss: 54.061227\n",
      "Epoch 86, loss: 54.050171\n",
      "Epoch 87, loss: 54.039168\n",
      "Epoch 88, loss: 54.028218\n",
      "Epoch 89, loss: 54.017320\n",
      "Epoch 90, loss: 54.006473\n",
      "Epoch 91, loss: 53.995678\n",
      "Epoch 92, loss: 53.984935\n",
      "Epoch 93, loss: 53.974242\n",
      "Epoch 94, loss: 53.963599\n",
      "Epoch 95, loss: 53.953007\n",
      "Epoch 96, loss: 53.942465\n",
      "Epoch 97, loss: 53.931972\n",
      "Epoch 98, loss: 53.921529\n",
      "Epoch 99, loss: 53.911134\n",
      "Epoch 100, loss: 53.900788\n",
      "Epoch 101, loss: 53.890490\n",
      "Epoch 102, loss: 53.880241\n",
      "Epoch 103, loss: 53.870039\n",
      "Epoch 104, loss: 53.859884\n",
      "Epoch 105, loss: 53.849776\n",
      "Epoch 106, loss: 53.839716\n",
      "Epoch 107, loss: 53.829701\n",
      "Epoch 108, loss: 53.819734\n",
      "Epoch 109, loss: 53.809812\n",
      "Epoch 110, loss: 53.799935\n",
      "Epoch 111, loss: 53.790104\n",
      "Epoch 112, loss: 53.780318\n",
      "Epoch 113, loss: 53.770578\n",
      "Epoch 114, loss: 53.760881\n",
      "Epoch 115, loss: 53.751229\n",
      "Epoch 116, loss: 53.741621\n",
      "Epoch 117, loss: 53.732057\n",
      "Epoch 118, loss: 53.722537\n",
      "Epoch 119, loss: 53.713059\n",
      "Epoch 120, loss: 53.703625\n",
      "Epoch 121, loss: 53.694234\n",
      "Epoch 122, loss: 53.684885\n",
      "Epoch 123, loss: 53.675578\n",
      "Epoch 124, loss: 53.666314\n",
      "Epoch 125, loss: 53.657091\n",
      "Epoch 126, loss: 53.647910\n",
      "Epoch 127, loss: 53.638770\n",
      "Epoch 128, loss: 53.629671\n",
      "Epoch 129, loss: 53.620613\n",
      "Epoch 130, loss: 53.611596\n",
      "Epoch 131, loss: 53.602619\n",
      "Epoch 132, loss: 53.593682\n",
      "Epoch 133, loss: 53.584785\n",
      "Epoch 134, loss: 53.575928\n",
      "Epoch 135, loss: 53.567110\n",
      "Epoch 136, loss: 53.558332\n",
      "Epoch 137, loss: 53.549593\n",
      "Epoch 138, loss: 53.540892\n",
      "Epoch 139, loss: 53.532230\n",
      "Epoch 140, loss: 53.523607\n",
      "Epoch 141, loss: 53.515022\n",
      "Epoch 142, loss: 53.506474\n",
      "Epoch 143, loss: 53.497965\n",
      "Epoch 144, loss: 53.489493\n",
      "Epoch 145, loss: 53.481058\n",
      "Epoch 146, loss: 53.472661\n",
      "Epoch 147, loss: 53.464300\n",
      "Epoch 148, loss: 53.455976\n",
      "Epoch 149, loss: 53.447689\n",
      "Epoch 150, loss: 53.439438\n",
      "Epoch 151, loss: 53.431223\n",
      "Epoch 152, loss: 53.423044\n",
      "Epoch 153, loss: 53.414901\n",
      "Epoch 154, loss: 53.406793\n",
      "Epoch 155, loss: 53.398721\n",
      "Epoch 156, loss: 53.390684\n",
      "Epoch 157, loss: 53.382681\n",
      "Epoch 158, loss: 53.374714\n",
      "Epoch 159, loss: 53.366781\n",
      "Epoch 160, loss: 53.358883\n",
      "Epoch 161, loss: 53.351018\n",
      "Epoch 162, loss: 53.343188\n",
      "Epoch 163, loss: 53.335392\n",
      "Epoch 164, loss: 53.327629\n",
      "Epoch 165, loss: 53.319900\n",
      "Epoch 166, loss: 53.312204\n",
      "Epoch 167, loss: 53.304541\n",
      "Epoch 168, loss: 53.296911\n",
      "Epoch 169, loss: 53.289314\n",
      "Epoch 170, loss: 53.281749\n",
      "Epoch 171, loss: 53.274217\n",
      "Epoch 172, loss: 53.266717\n",
      "Epoch 173, loss: 53.259249\n",
      "Epoch 174, loss: 53.251813\n",
      "Epoch 175, loss: 53.244409\n",
      "Epoch 176, loss: 53.237036\n",
      "Epoch 177, loss: 53.229694\n",
      "Epoch 178, loss: 53.222384\n",
      "Epoch 179, loss: 53.215105\n",
      "Epoch 180, loss: 53.207857\n",
      "Epoch 181, loss: 53.200639\n",
      "Epoch 182, loss: 53.193452\n",
      "Epoch 183, loss: 53.186295\n",
      "Epoch 184, loss: 53.179169\n",
      "Epoch 185, loss: 53.172073\n",
      "Epoch 186, loss: 53.165006\n",
      "Epoch 187, loss: 53.157969\n",
      "Epoch 188, loss: 53.150962\n",
      "Epoch 189, loss: 53.143984\n",
      "Epoch 190, loss: 53.137036\n",
      "Epoch 191, loss: 53.130116\n",
      "Epoch 192, loss: 53.123226\n",
      "Epoch 193, loss: 53.116364\n",
      "Epoch 194, loss: 53.109531\n",
      "Epoch 195, loss: 53.102727\n",
      "Epoch 196, loss: 53.095950\n",
      "Epoch 197, loss: 53.089202\n",
      "Epoch 198, loss: 53.082482\n",
      "Epoch 199, loss: 53.075790\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.268971\n",
      "Epoch 1, loss: 55.245789\n",
      "Epoch 2, loss: 55.224399\n",
      "Epoch 3, loss: 55.204278\n",
      "Epoch 4, loss: 55.185065\n",
      "Epoch 5, loss: 55.166513\n",
      "Epoch 6, loss: 55.148452\n",
      "Epoch 7, loss: 55.130762\n",
      "Epoch 8, loss: 55.113362\n",
      "Epoch 9, loss: 55.096196\n",
      "Epoch 10, loss: 55.079224\n",
      "Epoch 11, loss: 55.062419\n",
      "Epoch 12, loss: 55.045762\n",
      "Epoch 13, loss: 55.029238\n",
      "Epoch 14, loss: 55.012838\n",
      "Epoch 15, loss: 54.996554\n",
      "Epoch 16, loss: 54.980382\n",
      "Epoch 17, loss: 54.964317\n",
      "Epoch 18, loss: 54.948357\n",
      "Epoch 19, loss: 54.932498\n",
      "Epoch 20, loss: 54.916738\n",
      "Epoch 21, loss: 54.901076\n",
      "Epoch 22, loss: 54.885510\n",
      "Epoch 23, loss: 54.870039\n",
      "Epoch 24, loss: 54.854661\n",
      "Epoch 25, loss: 54.839376\n",
      "Epoch 26, loss: 54.824181\n",
      "Epoch 27, loss: 54.809077\n",
      "Epoch 28, loss: 54.794061\n",
      "Epoch 29, loss: 54.779134\n",
      "Epoch 30, loss: 54.764293\n",
      "Epoch 31, loss: 54.749538\n",
      "Epoch 32, loss: 54.734868\n",
      "Epoch 33, loss: 54.720282\n",
      "Epoch 34, loss: 54.705779\n",
      "Epoch 35, loss: 54.691359\n",
      "Epoch 36, loss: 54.677020\n",
      "Epoch 37, loss: 54.662762\n",
      "Epoch 38, loss: 54.648583\n",
      "Epoch 39, loss: 54.634483\n",
      "Epoch 40, loss: 54.620462\n",
      "Epoch 41, loss: 54.606518\n",
      "Epoch 42, loss: 54.592650\n",
      "Epoch 43, loss: 54.578858\n",
      "Epoch 44, loss: 54.565142\n",
      "Epoch 45, loss: 54.551500\n",
      "Epoch 46, loss: 54.537931\n",
      "Epoch 47, loss: 54.524436\n",
      "Epoch 48, loss: 54.511013\n",
      "Epoch 49, loss: 54.497662\n",
      "Epoch 50, loss: 54.484381\n",
      "Epoch 51, loss: 54.471172\n",
      "Epoch 52, loss: 54.458031\n",
      "Epoch 53, loss: 54.444960\n",
      "Epoch 54, loss: 54.431958\n",
      "Epoch 55, loss: 54.419023\n",
      "Epoch 56, loss: 54.406156\n",
      "Epoch 57, loss: 54.393355\n",
      "Epoch 58, loss: 54.380621\n",
      "Epoch 59, loss: 54.367952\n",
      "Epoch 60, loss: 54.355348\n",
      "Epoch 61, loss: 54.342809\n",
      "Epoch 62, loss: 54.330334\n",
      "Epoch 63, loss: 54.317922\n",
      "Epoch 64, loss: 54.305574\n",
      "Epoch 65, loss: 54.293287\n",
      "Epoch 66, loss: 54.281063\n",
      "Epoch 67, loss: 54.268900\n",
      "Epoch 68, loss: 54.256799\n",
      "Epoch 69, loss: 54.244757\n",
      "Epoch 70, loss: 54.232776\n",
      "Epoch 71, loss: 54.220854\n",
      "Epoch 72, loss: 54.208992\n",
      "Epoch 73, loss: 54.197188\n",
      "Epoch 74, loss: 54.185442\n",
      "Epoch 75, loss: 54.173755\n",
      "Epoch 76, loss: 54.162124\n",
      "Epoch 77, loss: 54.150551\n",
      "Epoch 78, loss: 54.139034\n",
      "Epoch 79, loss: 54.127574\n",
      "Epoch 80, loss: 54.116169\n",
      "Epoch 81, loss: 54.104819\n",
      "Epoch 82, loss: 54.093525\n",
      "Epoch 83, loss: 54.082285\n",
      "Epoch 84, loss: 54.071099\n",
      "Epoch 85, loss: 54.059967\n",
      "Epoch 86, loss: 54.048888\n",
      "Epoch 87, loss: 54.037863\n",
      "Epoch 88, loss: 54.026890\n",
      "Epoch 89, loss: 54.015970\n",
      "Epoch 90, loss: 54.005102\n",
      "Epoch 91, loss: 53.994285\n",
      "Epoch 92, loss: 53.983520\n",
      "Epoch 93, loss: 53.972805\n",
      "Epoch 94, loss: 53.962142\n",
      "Epoch 95, loss: 53.951528\n",
      "Epoch 96, loss: 53.940965\n",
      "Epoch 97, loss: 53.930451\n",
      "Epoch 98, loss: 53.919987\n",
      "Epoch 99, loss: 53.909572\n",
      "Epoch 100, loss: 53.899205\n",
      "Epoch 101, loss: 53.888887\n",
      "Epoch 102, loss: 53.878617\n",
      "Epoch 103, loss: 53.868395\n",
      "Epoch 104, loss: 53.858220\n",
      "Epoch 105, loss: 53.848093\n",
      "Epoch 106, loss: 53.838012\n",
      "Epoch 107, loss: 53.827978\n",
      "Epoch 108, loss: 53.817991\n",
      "Epoch 109, loss: 53.808049\n",
      "Epoch 110, loss: 53.798154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111, loss: 53.788303\n",
      "Epoch 112, loss: 53.778498\n",
      "Epoch 113, loss: 53.768738\n",
      "Epoch 114, loss: 53.759023\n",
      "Epoch 115, loss: 53.749352\n",
      "Epoch 116, loss: 53.739725\n",
      "Epoch 117, loss: 53.730143\n",
      "Epoch 118, loss: 53.720603\n",
      "Epoch 119, loss: 53.711108\n",
      "Epoch 120, loss: 53.701655\n",
      "Epoch 121, loss: 53.692245\n",
      "Epoch 122, loss: 53.682878\n",
      "Epoch 123, loss: 53.673553\n",
      "Epoch 124, loss: 53.664270\n",
      "Epoch 125, loss: 53.655030\n",
      "Epoch 126, loss: 53.645830\n",
      "Epoch 127, loss: 53.636673\n",
      "Epoch 128, loss: 53.627556\n",
      "Epoch 129, loss: 53.618481\n",
      "Epoch 130, loss: 53.609446\n",
      "Epoch 131, loss: 53.600451\n",
      "Epoch 132, loss: 53.591497\n",
      "Epoch 133, loss: 53.582583\n",
      "Epoch 134, loss: 53.573708\n",
      "Epoch 135, loss: 53.564874\n",
      "Epoch 136, loss: 53.556078\n",
      "Epoch 137, loss: 53.547322\n",
      "Epoch 138, loss: 53.538604\n",
      "Epoch 139, loss: 53.529925\n",
      "Epoch 140, loss: 53.521285\n",
      "Epoch 141, loss: 53.512683\n",
      "Epoch 142, loss: 53.504119\n",
      "Epoch 143, loss: 53.495593\n",
      "Epoch 144, loss: 53.487104\n",
      "Epoch 145, loss: 53.478653\n",
      "Epoch 146, loss: 53.470239\n",
      "Epoch 147, loss: 53.461862\n",
      "Epoch 148, loss: 53.453521\n",
      "Epoch 149, loss: 53.445218\n",
      "Epoch 150, loss: 53.436950\n",
      "Epoch 151, loss: 53.428719\n",
      "Epoch 152, loss: 53.420524\n",
      "Epoch 153, loss: 53.412365\n",
      "Epoch 154, loss: 53.404241\n",
      "Epoch 155, loss: 53.396153\n",
      "Epoch 156, loss: 53.388100\n",
      "Epoch 157, loss: 53.380082\n",
      "Epoch 158, loss: 53.372099\n",
      "Epoch 159, loss: 53.364150\n",
      "Epoch 160, loss: 53.356236\n",
      "Epoch 161, loss: 53.348356\n",
      "Epoch 162, loss: 53.340510\n",
      "Epoch 163, loss: 53.332698\n",
      "Epoch 164, loss: 53.324920\n",
      "Epoch 165, loss: 53.317175\n",
      "Epoch 166, loss: 53.309464\n",
      "Epoch 167, loss: 53.301786\n",
      "Epoch 168, loss: 53.294140\n",
      "Epoch 169, loss: 53.286528\n",
      "Epoch 170, loss: 53.278948\n",
      "Epoch 171, loss: 53.271401\n",
      "Epoch 172, loss: 53.263886\n",
      "Epoch 173, loss: 53.256403\n",
      "Epoch 174, loss: 53.248952\n",
      "Epoch 175, loss: 53.241533\n",
      "Epoch 176, loss: 53.234145\n",
      "Epoch 177, loss: 53.226789\n",
      "Epoch 178, loss: 53.219464\n",
      "Epoch 179, loss: 53.212170\n",
      "Epoch 180, loss: 53.204907\n",
      "Epoch 181, loss: 53.197674\n",
      "Epoch 182, loss: 53.190473\n",
      "Epoch 183, loss: 53.183302\n",
      "Epoch 184, loss: 53.176161\n",
      "Epoch 185, loss: 53.169050\n",
      "Epoch 186, loss: 53.161969\n",
      "Epoch 187, loss: 53.154917\n",
      "Epoch 188, loss: 53.147896\n",
      "Epoch 189, loss: 53.140904\n",
      "Epoch 190, loss: 53.133941\n",
      "Epoch 191, loss: 53.127007\n",
      "Epoch 192, loss: 53.120103\n",
      "Epoch 193, loss: 53.113227\n",
      "Epoch 194, loss: 53.106380\n",
      "Epoch 195, loss: 53.099561\n",
      "Epoch 196, loss: 53.092771\n",
      "Epoch 197, loss: 53.086009\n",
      "Epoch 198, loss: 53.079275\n",
      "Epoch 199, loss: 53.072569\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.261912\n",
      "Epoch 1, loss: 55.238985\n",
      "Epoch 2, loss: 55.217783\n",
      "Epoch 3, loss: 55.197804\n",
      "Epoch 4, loss: 55.178701\n",
      "Epoch 5, loss: 55.160236\n",
      "Epoch 6, loss: 55.142245\n",
      "Epoch 7, loss: 55.124615\n",
      "Epoch 8, loss: 55.107269\n",
      "Epoch 9, loss: 55.090151\n",
      "Epoch 10, loss: 55.073223\n",
      "Epoch 11, loss: 55.056461\n",
      "Epoch 12, loss: 55.039843\n",
      "Epoch 13, loss: 55.023359\n",
      "Epoch 14, loss: 55.006997\n",
      "Epoch 15, loss: 54.990752\n",
      "Epoch 16, loss: 54.974617\n",
      "Epoch 17, loss: 54.958590\n",
      "Epoch 18, loss: 54.942666\n",
      "Epoch 19, loss: 54.926844\n",
      "Epoch 20, loss: 54.911121\n",
      "Epoch 21, loss: 54.895496\n",
      "Epoch 22, loss: 54.879967\n",
      "Epoch 23, loss: 54.864532\n",
      "Epoch 24, loss: 54.849191\n",
      "Epoch 25, loss: 54.833941\n",
      "Epoch 26, loss: 54.818783\n",
      "Epoch 27, loss: 54.803714\n",
      "Epoch 28, loss: 54.788735\n",
      "Epoch 29, loss: 54.773843\n",
      "Epoch 30, loss: 54.759037\n",
      "Epoch 31, loss: 54.744318\n",
      "Epoch 32, loss: 54.729684\n",
      "Epoch 33, loss: 54.715133\n",
      "Epoch 34, loss: 54.700665\n",
      "Epoch 35, loss: 54.686280\n",
      "Epoch 36, loss: 54.671976\n",
      "Epoch 37, loss: 54.657752\n",
      "Epoch 38, loss: 54.643609\n",
      "Epoch 39, loss: 54.629543\n",
      "Epoch 40, loss: 54.615556\n",
      "Epoch 41, loss: 54.601646\n",
      "Epoch 42, loss: 54.587813\n",
      "Epoch 43, loss: 54.574055\n",
      "Epoch 44, loss: 54.560373\n",
      "Epoch 45, loss: 54.546764\n",
      "Epoch 46, loss: 54.533230\n",
      "Epoch 47, loss: 54.519768\n",
      "Epoch 48, loss: 54.506378\n",
      "Epoch 49, loss: 54.493060\n",
      "Epoch 50, loss: 54.479812\n",
      "Epoch 51, loss: 54.466635\n",
      "Epoch 52, loss: 54.453528\n",
      "Epoch 53, loss: 54.440489\n",
      "Epoch 54, loss: 54.427519\n",
      "Epoch 55, loss: 54.414617\n",
      "Epoch 56, loss: 54.401782\n",
      "Epoch 57, loss: 54.389013\n",
      "Epoch 58, loss: 54.376310\n",
      "Epoch 59, loss: 54.363673\n",
      "Epoch 60, loss: 54.351101\n",
      "Epoch 61, loss: 54.338593\n",
      "Epoch 62, loss: 54.326149\n",
      "Epoch 63, loss: 54.313769\n",
      "Epoch 64, loss: 54.301451\n",
      "Epoch 65, loss: 54.289195\n",
      "Epoch 66, loss: 54.277002\n",
      "Epoch 67, loss: 54.264869\n",
      "Epoch 68, loss: 54.252798\n",
      "Epoch 69, loss: 54.240787\n",
      "Epoch 70, loss: 54.228835\n",
      "Epoch 71, loss: 54.216943\n",
      "Epoch 72, loss: 54.205110\n",
      "Epoch 73, loss: 54.193336\n",
      "Epoch 74, loss: 54.181620\n",
      "Epoch 75, loss: 54.169961\n",
      "Epoch 76, loss: 54.158360\n",
      "Epoch 77, loss: 54.146815\n",
      "Epoch 78, loss: 54.135327\n",
      "Epoch 79, loss: 54.123895\n",
      "Epoch 80, loss: 54.112519\n",
      "Epoch 81, loss: 54.101197\n",
      "Epoch 82, loss: 54.089931\n",
      "Epoch 83, loss: 54.078719\n",
      "Epoch 84, loss: 54.067560\n",
      "Epoch 85, loss: 54.056456\n",
      "Epoch 86, loss: 54.045405\n",
      "Epoch 87, loss: 54.034407\n",
      "Epoch 88, loss: 54.023461\n",
      "Epoch 89, loss: 54.012568\n",
      "Epoch 90, loss: 54.001726\n",
      "Epoch 91, loss: 53.990936\n",
      "Epoch 92, loss: 53.980197\n",
      "Epoch 93, loss: 53.969509\n",
      "Epoch 94, loss: 53.958872\n",
      "Epoch 95, loss: 53.948284\n",
      "Epoch 96, loss: 53.937747\n",
      "Epoch 97, loss: 53.927259\n",
      "Epoch 98, loss: 53.916820\n",
      "Epoch 99, loss: 53.906430\n",
      "Epoch 100, loss: 53.896088\n",
      "Epoch 101, loss: 53.885795\n",
      "Epoch 102, loss: 53.875550\n",
      "Epoch 103, loss: 53.865353\n",
      "Epoch 104, loss: 53.855202\n",
      "Epoch 105, loss: 53.845099\n",
      "Epoch 106, loss: 53.835043\n",
      "Epoch 107, loss: 53.825033\n",
      "Epoch 108, loss: 53.815070\n",
      "Epoch 109, loss: 53.805152\n",
      "Epoch 110, loss: 53.795280\n",
      "Epoch 111, loss: 53.785453\n",
      "Epoch 112, loss: 53.775671\n",
      "Epoch 113, loss: 53.765934\n",
      "Epoch 114, loss: 53.756242\n",
      "Epoch 115, loss: 53.746594\n",
      "Epoch 116, loss: 53.736990\n",
      "Epoch 117, loss: 53.727430\n",
      "Epoch 118, loss: 53.717913\n",
      "Epoch 119, loss: 53.708439\n",
      "Epoch 120, loss: 53.699009\n",
      "Epoch 121, loss: 53.689621\n",
      "Epoch 122, loss: 53.680276\n",
      "Epoch 123, loss: 53.670972\n",
      "Epoch 124, loss: 53.661711\n",
      "Epoch 125, loss: 53.652492\n",
      "Epoch 126, loss: 53.643314\n",
      "Epoch 127, loss: 53.634177\n",
      "Epoch 128, loss: 53.625082\n",
      "Epoch 129, loss: 53.616027\n",
      "Epoch 130, loss: 53.607013\n",
      "Epoch 131, loss: 53.598039\n",
      "Epoch 132, loss: 53.589105\n",
      "Epoch 133, loss: 53.580211\n",
      "Epoch 134, loss: 53.571357\n",
      "Epoch 135, loss: 53.562542\n",
      "Epoch 136, loss: 53.553766\n",
      "Epoch 137, loss: 53.545029\n",
      "Epoch 138, loss: 53.536331\n",
      "Epoch 139, loss: 53.527672\n",
      "Epoch 140, loss: 53.519051\n",
      "Epoch 141, loss: 53.510468\n",
      "Epoch 142, loss: 53.501923\n",
      "Epoch 143, loss: 53.493415\n",
      "Epoch 144, loss: 53.484945\n",
      "Epoch 145, loss: 53.476513\n",
      "Epoch 146, loss: 53.468117\n",
      "Epoch 147, loss: 53.459759\n",
      "Epoch 148, loss: 53.451437\n",
      "Epoch 149, loss: 53.443151\n",
      "Epoch 150, loss: 53.434902\n",
      "Epoch 151, loss: 53.426688\n",
      "Epoch 152, loss: 53.418511\n",
      "Epoch 153, loss: 53.410369\n",
      "Epoch 154, loss: 53.402263\n",
      "Epoch 155, loss: 53.394192\n",
      "Epoch 156, loss: 53.386156\n",
      "Epoch 157, loss: 53.378155\n",
      "Epoch 158, loss: 53.370189\n",
      "Epoch 159, loss: 53.362257\n",
      "Epoch 160, loss: 53.354359\n",
      "Epoch 161, loss: 53.346496\n",
      "Epoch 162, loss: 53.338667\n",
      "Epoch 163, loss: 53.330871\n",
      "Epoch 164, loss: 53.323109\n",
      "Epoch 165, loss: 53.315380\n",
      "Epoch 166, loss: 53.307685\n",
      "Epoch 167, loss: 53.300022\n",
      "Epoch 168, loss: 53.292393\n",
      "Epoch 169, loss: 53.284796\n",
      "Epoch 170, loss: 53.277232\n",
      "Epoch 171, loss: 53.269700\n",
      "Epoch 172, loss: 53.262200\n",
      "Epoch 173, loss: 53.254732\n",
      "Epoch 174, loss: 53.247296\n",
      "Epoch 175, loss: 53.239892\n",
      "Epoch 176, loss: 53.232519\n",
      "Epoch 177, loss: 53.225177\n",
      "Epoch 178, loss: 53.217867\n",
      "Epoch 179, loss: 53.210587\n",
      "Epoch 180, loss: 53.203339\n",
      "Epoch 181, loss: 53.196120\n",
      "Epoch 182, loss: 53.188933\n",
      "Epoch 183, loss: 53.181776\n",
      "Epoch 184, loss: 53.174649\n",
      "Epoch 185, loss: 53.167551\n",
      "Epoch 186, loss: 53.160484\n",
      "Epoch 187, loss: 53.153447\n",
      "Epoch 188, loss: 53.146438\n",
      "Epoch 189, loss: 53.139460\n",
      "Epoch 190, loss: 53.132510\n",
      "Epoch 191, loss: 53.125590\n",
      "Epoch 192, loss: 53.118698\n",
      "Epoch 193, loss: 53.111835\n",
      "Epoch 194, loss: 53.105001\n",
      "Epoch 195, loss: 53.098195\n",
      "Epoch 196, loss: 53.091417\n",
      "Epoch 197, loss: 53.084668\n",
      "Epoch 198, loss: 53.077946\n",
      "Epoch 199, loss: 53.071252\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.263313\n",
      "Epoch 1, loss: 55.260962\n",
      "Epoch 2, loss: 55.258632\n",
      "Epoch 3, loss: 55.256322\n",
      "Epoch 4, loss: 55.254031\n",
      "Epoch 5, loss: 55.251758\n",
      "Epoch 6, loss: 55.249504\n",
      "Epoch 7, loss: 55.247266\n",
      "Epoch 8, loss: 55.245046\n",
      "Epoch 9, loss: 55.242842\n",
      "Epoch 10, loss: 55.240654\n",
      "Epoch 11, loss: 55.238481\n",
      "Epoch 12, loss: 55.236323\n",
      "Epoch 13, loss: 55.234179\n",
      "Epoch 14, loss: 55.232049\n",
      "Epoch 15, loss: 55.229933\n",
      "Epoch 16, loss: 55.227829\n",
      "Epoch 17, loss: 55.225739\n",
      "Epoch 18, loss: 55.223660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 55.221594\n",
      "Epoch 20, loss: 55.219539\n",
      "Epoch 21, loss: 55.217495\n",
      "Epoch 22, loss: 55.215462\n",
      "Epoch 23, loss: 55.213440\n",
      "Epoch 24, loss: 55.211428\n",
      "Epoch 25, loss: 55.209425\n",
      "Epoch 26, loss: 55.207432\n",
      "Epoch 27, loss: 55.205449\n",
      "Epoch 28, loss: 55.203475\n",
      "Epoch 29, loss: 55.201509\n",
      "Epoch 30, loss: 55.199552\n",
      "Epoch 31, loss: 55.197603\n",
      "Epoch 32, loss: 55.195662\n",
      "Epoch 33, loss: 55.193729\n",
      "Epoch 34, loss: 55.191803\n",
      "Epoch 35, loss: 55.189885\n",
      "Epoch 36, loss: 55.187974\n",
      "Epoch 37, loss: 55.186069\n",
      "Epoch 38, loss: 55.184172\n",
      "Epoch 39, loss: 55.182281\n",
      "Epoch 40, loss: 55.180397\n",
      "Epoch 41, loss: 55.178518\n",
      "Epoch 42, loss: 55.176646\n",
      "Epoch 43, loss: 55.174779\n",
      "Epoch 44, loss: 55.172918\n",
      "Epoch 45, loss: 55.171063\n",
      "Epoch 46, loss: 55.169213\n",
      "Epoch 47, loss: 55.167368\n",
      "Epoch 48, loss: 55.165529\n",
      "Epoch 49, loss: 55.163694\n",
      "Epoch 50, loss: 55.161864\n",
      "Epoch 51, loss: 55.160039\n",
      "Epoch 52, loss: 55.158218\n",
      "Epoch 53, loss: 55.156402\n",
      "Epoch 54, loss: 55.154591\n",
      "Epoch 55, loss: 55.152783\n",
      "Epoch 56, loss: 55.150980\n",
      "Epoch 57, loss: 55.149180\n",
      "Epoch 58, loss: 55.147385\n",
      "Epoch 59, loss: 55.145594\n",
      "Epoch 60, loss: 55.143806\n",
      "Epoch 61, loss: 55.142022\n",
      "Epoch 62, loss: 55.140241\n",
      "Epoch 63, loss: 55.138464\n",
      "Epoch 64, loss: 55.136690\n",
      "Epoch 65, loss: 55.134920\n",
      "Epoch 66, loss: 55.133153\n",
      "Epoch 67, loss: 55.131389\n",
      "Epoch 68, loss: 55.129628\n",
      "Epoch 69, loss: 55.127870\n",
      "Epoch 70, loss: 55.126116\n",
      "Epoch 71, loss: 55.124364\n",
      "Epoch 72, loss: 55.122615\n",
      "Epoch 73, loss: 55.120868\n",
      "Epoch 74, loss: 55.119125\n",
      "Epoch 75, loss: 55.117384\n",
      "Epoch 76, loss: 55.115646\n",
      "Epoch 77, loss: 55.113910\n",
      "Epoch 78, loss: 55.112177\n",
      "Epoch 79, loss: 55.110446\n",
      "Epoch 80, loss: 55.108718\n",
      "Epoch 81, loss: 55.106992\n",
      "Epoch 82, loss: 55.105269\n",
      "Epoch 83, loss: 55.103548\n",
      "Epoch 84, loss: 55.101829\n",
      "Epoch 85, loss: 55.100112\n",
      "Epoch 86, loss: 55.098397\n",
      "Epoch 87, loss: 55.096685\n",
      "Epoch 88, loss: 55.094975\n",
      "Epoch 89, loss: 55.093266\n",
      "Epoch 90, loss: 55.091560\n",
      "Epoch 91, loss: 55.089856\n",
      "Epoch 92, loss: 55.088153\n",
      "Epoch 93, loss: 55.086453\n",
      "Epoch 94, loss: 55.084755\n",
      "Epoch 95, loss: 55.083058\n",
      "Epoch 96, loss: 55.081363\n",
      "Epoch 97, loss: 55.079670\n",
      "Epoch 98, loss: 55.077979\n",
      "Epoch 99, loss: 55.076290\n",
      "Epoch 100, loss: 55.074602\n",
      "Epoch 101, loss: 55.072916\n",
      "Epoch 102, loss: 55.071232\n",
      "Epoch 103, loss: 55.069550\n",
      "Epoch 104, loss: 55.067869\n",
      "Epoch 105, loss: 55.066190\n",
      "Epoch 106, loss: 55.064512\n",
      "Epoch 107, loss: 55.062836\n",
      "Epoch 108, loss: 55.061162\n",
      "Epoch 109, loss: 55.059489\n",
      "Epoch 110, loss: 55.057817\n",
      "Epoch 111, loss: 55.056148\n",
      "Epoch 112, loss: 55.054479\n",
      "Epoch 113, loss: 55.052813\n",
      "Epoch 114, loss: 55.051147\n",
      "Epoch 115, loss: 55.049483\n",
      "Epoch 116, loss: 55.047821\n",
      "Epoch 117, loss: 55.046160\n",
      "Epoch 118, loss: 55.044500\n",
      "Epoch 119, loss: 55.042842\n",
      "Epoch 120, loss: 55.041185\n",
      "Epoch 121, loss: 55.039530\n",
      "Epoch 122, loss: 55.037876\n",
      "Epoch 123, loss: 55.036223\n",
      "Epoch 124, loss: 55.034572\n",
      "Epoch 125, loss: 55.032922\n",
      "Epoch 126, loss: 55.031273\n",
      "Epoch 127, loss: 55.029626\n",
      "Epoch 128, loss: 55.027980\n",
      "Epoch 129, loss: 55.026335\n",
      "Epoch 130, loss: 55.024691\n",
      "Epoch 131, loss: 55.023049\n",
      "Epoch 132, loss: 55.021408\n",
      "Epoch 133, loss: 55.019769\n",
      "Epoch 134, loss: 55.018130\n",
      "Epoch 135, loss: 55.016493\n",
      "Epoch 136, loss: 55.014857\n",
      "Epoch 137, loss: 55.013222\n",
      "Epoch 138, loss: 55.011589\n",
      "Epoch 139, loss: 55.009956\n",
      "Epoch 140, loss: 55.008325\n",
      "Epoch 141, loss: 55.006695\n",
      "Epoch 142, loss: 55.005066\n",
      "Epoch 143, loss: 55.003439\n",
      "Epoch 144, loss: 55.001812\n",
      "Epoch 145, loss: 55.000187\n",
      "Epoch 146, loss: 54.998563\n",
      "Epoch 147, loss: 54.996940\n",
      "Epoch 148, loss: 54.995318\n",
      "Epoch 149, loss: 54.993698\n",
      "Epoch 150, loss: 54.992078\n",
      "Epoch 151, loss: 54.990460\n",
      "Epoch 152, loss: 54.988843\n",
      "Epoch 153, loss: 54.987226\n",
      "Epoch 154, loss: 54.985611\n",
      "Epoch 155, loss: 54.983998\n",
      "Epoch 156, loss: 54.982385\n",
      "Epoch 157, loss: 54.980773\n",
      "Epoch 158, loss: 54.979163\n",
      "Epoch 159, loss: 54.977553\n",
      "Epoch 160, loss: 54.975945\n",
      "Epoch 161, loss: 54.974337\n",
      "Epoch 162, loss: 54.972731\n",
      "Epoch 163, loss: 54.971126\n",
      "Epoch 164, loss: 54.969522\n",
      "Epoch 165, loss: 54.967919\n",
      "Epoch 166, loss: 54.966317\n",
      "Epoch 167, loss: 54.964716\n",
      "Epoch 168, loss: 54.963117\n",
      "Epoch 169, loss: 54.961518\n",
      "Epoch 170, loss: 54.959920\n",
      "Epoch 171, loss: 54.958324\n",
      "Epoch 172, loss: 54.956728\n",
      "Epoch 173, loss: 54.955134\n",
      "Epoch 174, loss: 54.953540\n",
      "Epoch 175, loss: 54.951948\n",
      "Epoch 176, loss: 54.950356\n",
      "Epoch 177, loss: 54.948766\n",
      "Epoch 178, loss: 54.947177\n",
      "Epoch 179, loss: 54.945588\n",
      "Epoch 180, loss: 54.944001\n",
      "Epoch 181, loss: 54.942415\n",
      "Epoch 182, loss: 54.940830\n",
      "Epoch 183, loss: 54.939245\n",
      "Epoch 184, loss: 54.937662\n",
      "Epoch 185, loss: 54.936080\n",
      "Epoch 186, loss: 54.934499\n",
      "Epoch 187, loss: 54.932919\n",
      "Epoch 188, loss: 54.931340\n",
      "Epoch 189, loss: 54.929761\n",
      "Epoch 190, loss: 54.928184\n",
      "Epoch 191, loss: 54.926608\n",
      "Epoch 192, loss: 54.925033\n",
      "Epoch 193, loss: 54.923459\n",
      "Epoch 194, loss: 54.921886\n",
      "Epoch 195, loss: 54.920314\n",
      "Epoch 196, loss: 54.918742\n",
      "Epoch 197, loss: 54.917172\n",
      "Epoch 198, loss: 54.915603\n",
      "Epoch 199, loss: 54.914035\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.262531\n",
      "Epoch 1, loss: 55.260301\n",
      "Epoch 2, loss: 55.258087\n",
      "Epoch 3, loss: 55.255888\n",
      "Epoch 4, loss: 55.253705\n",
      "Epoch 5, loss: 55.251537\n",
      "Epoch 6, loss: 55.249383\n",
      "Epoch 7, loss: 55.247244\n",
      "Epoch 8, loss: 55.245117\n",
      "Epoch 9, loss: 55.243004\n",
      "Epoch 10, loss: 55.240904\n",
      "Epoch 11, loss: 55.238815\n",
      "Epoch 12, loss: 55.236739\n",
      "Epoch 13, loss: 55.234674\n",
      "Epoch 14, loss: 55.232621\n",
      "Epoch 15, loss: 55.230578\n",
      "Epoch 16, loss: 55.228546\n",
      "Epoch 17, loss: 55.226524\n",
      "Epoch 18, loss: 55.224513\n",
      "Epoch 19, loss: 55.222510\n",
      "Epoch 20, loss: 55.220517\n",
      "Epoch 21, loss: 55.218533\n",
      "Epoch 22, loss: 55.216558\n",
      "Epoch 23, loss: 55.214592\n",
      "Epoch 24, loss: 55.212633\n",
      "Epoch 25, loss: 55.210683\n",
      "Epoch 26, loss: 55.208741\n",
      "Epoch 27, loss: 55.206806\n",
      "Epoch 28, loss: 55.204879\n",
      "Epoch 29, loss: 55.202958\n",
      "Epoch 30, loss: 55.201045\n",
      "Epoch 31, loss: 55.199138\n",
      "Epoch 32, loss: 55.197238\n",
      "Epoch 33, loss: 55.195345\n",
      "Epoch 34, loss: 55.193457\n",
      "Epoch 35, loss: 55.191576\n",
      "Epoch 36, loss: 55.189700\n",
      "Epoch 37, loss: 55.187830\n",
      "Epoch 38, loss: 55.185966\n",
      "Epoch 39, loss: 55.184107\n",
      "Epoch 40, loss: 55.182254\n",
      "Epoch 41, loss: 55.180405\n",
      "Epoch 42, loss: 55.178562\n",
      "Epoch 43, loss: 55.176723\n",
      "Epoch 44, loss: 55.174889\n",
      "Epoch 45, loss: 55.173060\n",
      "Epoch 46, loss: 55.171235\n",
      "Epoch 47, loss: 55.169415\n",
      "Epoch 48, loss: 55.167599\n",
      "Epoch 49, loss: 55.165787\n",
      "Epoch 50, loss: 55.163979\n",
      "Epoch 51, loss: 55.162175\n",
      "Epoch 52, loss: 55.160375\n",
      "Epoch 53, loss: 55.158578\n",
      "Epoch 54, loss: 55.156786\n",
      "Epoch 55, loss: 55.154996\n",
      "Epoch 56, loss: 55.153211\n",
      "Epoch 57, loss: 55.151429\n",
      "Epoch 58, loss: 55.149650\n",
      "Epoch 59, loss: 55.147874\n",
      "Epoch 60, loss: 55.146102\n",
      "Epoch 61, loss: 55.144333\n",
      "Epoch 62, loss: 55.142567\n",
      "Epoch 63, loss: 55.140803\n",
      "Epoch 64, loss: 55.139043\n",
      "Epoch 65, loss: 55.137286\n",
      "Epoch 66, loss: 55.135531\n",
      "Epoch 67, loss: 55.133779\n",
      "Epoch 68, loss: 55.132030\n",
      "Epoch 69, loss: 55.130284\n",
      "Epoch 70, loss: 55.128540\n",
      "Epoch 71, loss: 55.126798\n",
      "Epoch 72, loss: 55.125060\n",
      "Epoch 73, loss: 55.123323\n",
      "Epoch 74, loss: 55.121589\n",
      "Epoch 75, loss: 55.119857\n",
      "Epoch 76, loss: 55.118128\n",
      "Epoch 77, loss: 55.116401\n",
      "Epoch 78, loss: 55.114676\n",
      "Epoch 79, loss: 55.112953\n",
      "Epoch 80, loss: 55.111232\n",
      "Epoch 81, loss: 55.109514\n",
      "Epoch 82, loss: 55.107798\n",
      "Epoch 83, loss: 55.106083\n",
      "Epoch 84, loss: 55.104371\n",
      "Epoch 85, loss: 55.102660\n",
      "Epoch 86, loss: 55.100952\n",
      "Epoch 87, loss: 55.099245\n",
      "Epoch 88, loss: 55.097541\n",
      "Epoch 89, loss: 55.095838\n",
      "Epoch 90, loss: 55.094137\n",
      "Epoch 91, loss: 55.092438\n",
      "Epoch 92, loss: 55.090741\n",
      "Epoch 93, loss: 55.089045\n",
      "Epoch 94, loss: 55.087351\n",
      "Epoch 95, loss: 55.085659\n",
      "Epoch 96, loss: 55.083968\n",
      "Epoch 97, loss: 55.082280\n",
      "Epoch 98, loss: 55.080592\n",
      "Epoch 99, loss: 55.078907\n",
      "Epoch 100, loss: 55.077223\n",
      "Epoch 101, loss: 55.075541\n",
      "Epoch 102, loss: 55.073860\n",
      "Epoch 103, loss: 55.072180\n",
      "Epoch 104, loss: 55.070503\n",
      "Epoch 105, loss: 55.068827\n",
      "Epoch 106, loss: 55.067152\n",
      "Epoch 107, loss: 55.065479\n",
      "Epoch 108, loss: 55.063807\n",
      "Epoch 109, loss: 55.062137\n",
      "Epoch 110, loss: 55.060468\n",
      "Epoch 111, loss: 55.058800\n",
      "Epoch 112, loss: 55.057134\n",
      "Epoch 113, loss: 55.055470\n",
      "Epoch 114, loss: 55.053806\n",
      "Epoch 115, loss: 55.052145\n",
      "Epoch 116, loss: 55.050484\n",
      "Epoch 117, loss: 55.048825\n",
      "Epoch 118, loss: 55.047167\n",
      "Epoch 119, loss: 55.045511\n",
      "Epoch 120, loss: 55.043855\n",
      "Epoch 121, loss: 55.042202\n",
      "Epoch 122, loss: 55.040549\n",
      "Epoch 123, loss: 55.038898\n",
      "Epoch 124, loss: 55.037248\n",
      "Epoch 125, loss: 55.035599\n",
      "Epoch 126, loss: 55.033952\n",
      "Epoch 127, loss: 55.032305\n",
      "Epoch 128, loss: 55.030660\n",
      "Epoch 129, loss: 55.029017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, loss: 55.027374\n",
      "Epoch 131, loss: 55.025733\n",
      "Epoch 132, loss: 55.024093\n",
      "Epoch 133, loss: 55.022454\n",
      "Epoch 134, loss: 55.020816\n",
      "Epoch 135, loss: 55.019180\n",
      "Epoch 136, loss: 55.017545\n",
      "Epoch 137, loss: 55.015911\n",
      "Epoch 138, loss: 55.014278\n",
      "Epoch 139, loss: 55.012646\n",
      "Epoch 140, loss: 55.011015\n",
      "Epoch 141, loss: 55.009386\n",
      "Epoch 142, loss: 55.007758\n",
      "Epoch 143, loss: 55.006131\n",
      "Epoch 144, loss: 55.004505\n",
      "Epoch 145, loss: 55.002880\n",
      "Epoch 146, loss: 55.001256\n",
      "Epoch 147, loss: 54.999633\n",
      "Epoch 148, loss: 54.998012\n",
      "Epoch 149, loss: 54.996392\n",
      "Epoch 150, loss: 54.994772\n",
      "Epoch 151, loss: 54.993154\n",
      "Epoch 152, loss: 54.991537\n",
      "Epoch 153, loss: 54.989921\n",
      "Epoch 154, loss: 54.988306\n",
      "Epoch 155, loss: 54.986693\n",
      "Epoch 156, loss: 54.985080\n",
      "Epoch 157, loss: 54.983468\n",
      "Epoch 158, loss: 54.981858\n",
      "Epoch 159, loss: 54.980248\n",
      "Epoch 160, loss: 54.978640\n",
      "Epoch 161, loss: 54.977033\n",
      "Epoch 162, loss: 54.975426\n",
      "Epoch 163, loss: 54.973821\n",
      "Epoch 164, loss: 54.972217\n",
      "Epoch 165, loss: 54.970614\n",
      "Epoch 166, loss: 54.969012\n",
      "Epoch 167, loss: 54.967411\n",
      "Epoch 168, loss: 54.965811\n",
      "Epoch 169, loss: 54.964212\n",
      "Epoch 170, loss: 54.962614\n",
      "Epoch 171, loss: 54.961018\n",
      "Epoch 172, loss: 54.959422\n",
      "Epoch 173, loss: 54.957827\n",
      "Epoch 174, loss: 54.956233\n",
      "Epoch 175, loss: 54.954641\n",
      "Epoch 176, loss: 54.953049\n",
      "Epoch 177, loss: 54.951459\n",
      "Epoch 178, loss: 54.949869\n",
      "Epoch 179, loss: 54.948280\n",
      "Epoch 180, loss: 54.946693\n",
      "Epoch 181, loss: 54.945106\n",
      "Epoch 182, loss: 54.943521\n",
      "Epoch 183, loss: 54.941936\n",
      "Epoch 184, loss: 54.940353\n",
      "Epoch 185, loss: 54.938770\n",
      "Epoch 186, loss: 54.937188\n",
      "Epoch 187, loss: 54.935608\n",
      "Epoch 188, loss: 54.934028\n",
      "Epoch 189, loss: 54.932450\n",
      "Epoch 190, loss: 54.930872\n",
      "Epoch 191, loss: 54.929296\n",
      "Epoch 192, loss: 54.927720\n",
      "Epoch 193, loss: 54.926146\n",
      "Epoch 194, loss: 54.924572\n",
      "Epoch 195, loss: 54.922999\n",
      "Epoch 196, loss: 54.921428\n",
      "Epoch 197, loss: 54.919857\n",
      "Epoch 198, loss: 54.918287\n",
      "Epoch 199, loss: 54.916719\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.260180\n",
      "Epoch 1, loss: 55.258032\n",
      "Epoch 2, loss: 55.255898\n",
      "Epoch 3, loss: 55.253776\n",
      "Epoch 4, loss: 55.251667\n",
      "Epoch 5, loss: 55.249571\n",
      "Epoch 6, loss: 55.247486\n",
      "Epoch 7, loss: 55.245413\n",
      "Epoch 8, loss: 55.243351\n",
      "Epoch 9, loss: 55.241300\n",
      "Epoch 10, loss: 55.239259\n",
      "Epoch 11, loss: 55.237229\n",
      "Epoch 12, loss: 55.235208\n",
      "Epoch 13, loss: 55.233197\n",
      "Epoch 14, loss: 55.231195\n",
      "Epoch 15, loss: 55.229202\n",
      "Epoch 16, loss: 55.227218\n",
      "Epoch 17, loss: 55.225243\n",
      "Epoch 18, loss: 55.223276\n",
      "Epoch 19, loss: 55.221317\n",
      "Epoch 20, loss: 55.219365\n",
      "Epoch 21, loss: 55.217422\n",
      "Epoch 22, loss: 55.215485\n",
      "Epoch 23, loss: 55.213556\n",
      "Epoch 24, loss: 55.211633\n",
      "Epoch 25, loss: 55.209718\n",
      "Epoch 26, loss: 55.207809\n",
      "Epoch 27, loss: 55.205906\n",
      "Epoch 28, loss: 55.204009\n",
      "Epoch 29, loss: 55.202119\n",
      "Epoch 30, loss: 55.200234\n",
      "Epoch 31, loss: 55.198355\n",
      "Epoch 32, loss: 55.196482\n",
      "Epoch 33, loss: 55.194614\n",
      "Epoch 34, loss: 55.192751\n",
      "Epoch 35, loss: 55.190893\n",
      "Epoch 36, loss: 55.189040\n",
      "Epoch 37, loss: 55.187192\n",
      "Epoch 38, loss: 55.185349\n",
      "Epoch 39, loss: 55.183511\n",
      "Epoch 40, loss: 55.181677\n",
      "Epoch 41, loss: 55.179847\n",
      "Epoch 42, loss: 55.178022\n",
      "Epoch 43, loss: 55.176200\n",
      "Epoch 44, loss: 55.174383\n",
      "Epoch 45, loss: 55.172570\n",
      "Epoch 46, loss: 55.170761\n",
      "Epoch 47, loss: 55.168955\n",
      "Epoch 48, loss: 55.167153\n",
      "Epoch 49, loss: 55.165355\n",
      "Epoch 50, loss: 55.163560\n",
      "Epoch 51, loss: 55.161768\n",
      "Epoch 52, loss: 55.159980\n",
      "Epoch 53, loss: 55.158195\n",
      "Epoch 54, loss: 55.156414\n",
      "Epoch 55, loss: 55.154635\n",
      "Epoch 56, loss: 55.152860\n",
      "Epoch 57, loss: 55.151088\n",
      "Epoch 58, loss: 55.149318\n",
      "Epoch 59, loss: 55.147551\n",
      "Epoch 60, loss: 55.145788\n",
      "Epoch 61, loss: 55.144027\n",
      "Epoch 62, loss: 55.142268\n",
      "Epoch 63, loss: 55.140513\n",
      "Epoch 64, loss: 55.138759\n",
      "Epoch 65, loss: 55.137009\n",
      "Epoch 66, loss: 55.135261\n",
      "Epoch 67, loss: 55.133515\n",
      "Epoch 68, loss: 55.131772\n",
      "Epoch 69, loss: 55.130031\n",
      "Epoch 70, loss: 55.128293\n",
      "Epoch 71, loss: 55.126556\n",
      "Epoch 72, loss: 55.124822\n",
      "Epoch 73, loss: 55.123090\n",
      "Epoch 74, loss: 55.121361\n",
      "Epoch 75, loss: 55.119633\n",
      "Epoch 76, loss: 55.117908\n",
      "Epoch 77, loss: 55.116184\n",
      "Epoch 78, loss: 55.114463\n",
      "Epoch 79, loss: 55.112743\n",
      "Epoch 80, loss: 55.111026\n",
      "Epoch 81, loss: 55.109310\n",
      "Epoch 82, loss: 55.107596\n",
      "Epoch 83, loss: 55.105885\n",
      "Epoch 84, loss: 55.104175\n",
      "Epoch 85, loss: 55.102467\n",
      "Epoch 86, loss: 55.100760\n",
      "Epoch 87, loss: 55.099056\n",
      "Epoch 88, loss: 55.097353\n",
      "Epoch 89, loss: 55.095652\n",
      "Epoch 90, loss: 55.093952\n",
      "Epoch 91, loss: 55.092255\n",
      "Epoch 92, loss: 55.090558\n",
      "Epoch 93, loss: 55.088864\n",
      "Epoch 94, loss: 55.087171\n",
      "Epoch 95, loss: 55.085480\n",
      "Epoch 96, loss: 55.083790\n",
      "Epoch 97, loss: 55.082102\n",
      "Epoch 98, loss: 55.080416\n",
      "Epoch 99, loss: 55.078731\n",
      "Epoch 100, loss: 55.077047\n",
      "Epoch 101, loss: 55.075365\n",
      "Epoch 102, loss: 55.073685\n",
      "Epoch 103, loss: 55.072005\n",
      "Epoch 104, loss: 55.070328\n",
      "Epoch 105, loss: 55.068652\n",
      "Epoch 106, loss: 55.066977\n",
      "Epoch 107, loss: 55.065304\n",
      "Epoch 108, loss: 55.063632\n",
      "Epoch 109, loss: 55.061961\n",
      "Epoch 110, loss: 55.060292\n",
      "Epoch 111, loss: 55.058624\n",
      "Epoch 112, loss: 55.056958\n",
      "Epoch 113, loss: 55.055293\n",
      "Epoch 114, loss: 55.053629\n",
      "Epoch 115, loss: 55.051966\n",
      "Epoch 116, loss: 55.050305\n",
      "Epoch 117, loss: 55.048645\n",
      "Epoch 118, loss: 55.046987\n",
      "Epoch 119, loss: 55.045330\n",
      "Epoch 120, loss: 55.043674\n",
      "Epoch 121, loss: 55.042019\n",
      "Epoch 122, loss: 55.040365\n",
      "Epoch 123, loss: 55.038713\n",
      "Epoch 124, loss: 55.037062\n",
      "Epoch 125, loss: 55.035412\n",
      "Epoch 126, loss: 55.033764\n",
      "Epoch 127, loss: 55.032117\n",
      "Epoch 128, loss: 55.030471\n",
      "Epoch 129, loss: 55.028826\n",
      "Epoch 130, loss: 55.027182\n",
      "Epoch 131, loss: 55.025540\n",
      "Epoch 132, loss: 55.023898\n",
      "Epoch 133, loss: 55.022258\n",
      "Epoch 134, loss: 55.020619\n",
      "Epoch 135, loss: 55.018981\n",
      "Epoch 136, loss: 55.017345\n",
      "Epoch 137, loss: 55.015709\n",
      "Epoch 138, loss: 55.014075\n",
      "Epoch 139, loss: 55.012442\n",
      "Epoch 140, loss: 55.010810\n",
      "Epoch 141, loss: 55.009179\n",
      "Epoch 142, loss: 55.007549\n",
      "Epoch 143, loss: 55.005921\n",
      "Epoch 144, loss: 55.004293\n",
      "Epoch 145, loss: 55.002667\n",
      "Epoch 146, loss: 55.001042\n",
      "Epoch 147, loss: 54.999418\n",
      "Epoch 148, loss: 54.997795\n",
      "Epoch 149, loss: 54.996173\n",
      "Epoch 150, loss: 54.994552\n",
      "Epoch 151, loss: 54.992932\n",
      "Epoch 152, loss: 54.991314\n",
      "Epoch 153, loss: 54.989696\n",
      "Epoch 154, loss: 54.988080\n",
      "Epoch 155, loss: 54.986464\n",
      "Epoch 156, loss: 54.984850\n",
      "Epoch 157, loss: 54.983237\n",
      "Epoch 158, loss: 54.981625\n",
      "Epoch 159, loss: 54.980014\n",
      "Epoch 160, loss: 54.978404\n",
      "Epoch 161, loss: 54.976795\n",
      "Epoch 162, loss: 54.975187\n",
      "Epoch 163, loss: 54.973580\n",
      "Epoch 164, loss: 54.971974\n",
      "Epoch 165, loss: 54.970370\n",
      "Epoch 166, loss: 54.968766\n",
      "Epoch 167, loss: 54.967163\n",
      "Epoch 168, loss: 54.965562\n",
      "Epoch 169, loss: 54.963961\n",
      "Epoch 170, loss: 54.962362\n",
      "Epoch 171, loss: 54.960763\n",
      "Epoch 172, loss: 54.959166\n",
      "Epoch 173, loss: 54.957569\n",
      "Epoch 174, loss: 54.955974\n",
      "Epoch 175, loss: 54.954379\n",
      "Epoch 176, loss: 54.952786\n",
      "Epoch 177, loss: 54.951194\n",
      "Epoch 178, loss: 54.949603\n",
      "Epoch 179, loss: 54.948012\n",
      "Epoch 180, loss: 54.946423\n",
      "Epoch 181, loss: 54.944835\n",
      "Epoch 182, loss: 54.943247\n",
      "Epoch 183, loss: 54.941661\n",
      "Epoch 184, loss: 54.940076\n",
      "Epoch 185, loss: 54.938492\n",
      "Epoch 186, loss: 54.936908\n",
      "Epoch 187, loss: 54.935326\n",
      "Epoch 188, loss: 54.933745\n",
      "Epoch 189, loss: 54.932165\n",
      "Epoch 190, loss: 54.930585\n",
      "Epoch 191, loss: 54.929007\n",
      "Epoch 192, loss: 54.927430\n",
      "Epoch 193, loss: 54.925854\n",
      "Epoch 194, loss: 54.924278\n",
      "Epoch 195, loss: 54.922704\n",
      "Epoch 196, loss: 54.921131\n",
      "Epoch 197, loss: 54.919558\n",
      "Epoch 198, loss: 54.917987\n",
      "Epoch 199, loss: 54.916417\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.263477\n",
      "Epoch 1, loss: 55.263248\n",
      "Epoch 2, loss: 55.263019\n",
      "Epoch 3, loss: 55.262790\n",
      "Epoch 4, loss: 55.262562\n",
      "Epoch 5, loss: 55.262333\n",
      "Epoch 6, loss: 55.262105\n",
      "Epoch 7, loss: 55.261877\n",
      "Epoch 8, loss: 55.261649\n",
      "Epoch 9, loss: 55.261422\n",
      "Epoch 10, loss: 55.261194\n",
      "Epoch 11, loss: 55.260967\n",
      "Epoch 12, loss: 55.260740\n",
      "Epoch 13, loss: 55.260513\n",
      "Epoch 14, loss: 55.260286\n",
      "Epoch 15, loss: 55.260059\n",
      "Epoch 16, loss: 55.259833\n",
      "Epoch 17, loss: 55.259606\n",
      "Epoch 18, loss: 55.259380\n",
      "Epoch 19, loss: 55.259154\n",
      "Epoch 20, loss: 55.258929\n",
      "Epoch 21, loss: 55.258703\n",
      "Epoch 22, loss: 55.258478\n",
      "Epoch 23, loss: 55.258252\n",
      "Epoch 24, loss: 55.258027\n",
      "Epoch 25, loss: 55.257803\n",
      "Epoch 26, loss: 55.257578\n",
      "Epoch 27, loss: 55.257353\n",
      "Epoch 28, loss: 55.257129\n",
      "Epoch 29, loss: 55.256905\n",
      "Epoch 30, loss: 55.256681\n",
      "Epoch 31, loss: 55.256457\n",
      "Epoch 32, loss: 55.256233\n",
      "Epoch 33, loss: 55.256010\n",
      "Epoch 34, loss: 55.255786\n",
      "Epoch 35, loss: 55.255563\n",
      "Epoch 36, loss: 55.255340\n",
      "Epoch 37, loss: 55.255117\n",
      "Epoch 38, loss: 55.254895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, loss: 55.254672\n",
      "Epoch 40, loss: 55.254450\n",
      "Epoch 41, loss: 55.254228\n",
      "Epoch 42, loss: 55.254005\n",
      "Epoch 43, loss: 55.253784\n",
      "Epoch 44, loss: 55.253562\n",
      "Epoch 45, loss: 55.253340\n",
      "Epoch 46, loss: 55.253119\n",
      "Epoch 47, loss: 55.252898\n",
      "Epoch 48, loss: 55.252677\n",
      "Epoch 49, loss: 55.252456\n",
      "Epoch 50, loss: 55.252235\n",
      "Epoch 51, loss: 55.252014\n",
      "Epoch 52, loss: 55.251794\n",
      "Epoch 53, loss: 55.251574\n",
      "Epoch 54, loss: 55.251354\n",
      "Epoch 55, loss: 55.251134\n",
      "Epoch 56, loss: 55.250914\n",
      "Epoch 57, loss: 55.250694\n",
      "Epoch 58, loss: 55.250475\n",
      "Epoch 59, loss: 55.250255\n",
      "Epoch 60, loss: 55.250036\n",
      "Epoch 61, loss: 55.249817\n",
      "Epoch 62, loss: 55.249598\n",
      "Epoch 63, loss: 55.249380\n",
      "Epoch 64, loss: 55.249161\n",
      "Epoch 65, loss: 55.248943\n",
      "Epoch 66, loss: 55.248724\n",
      "Epoch 67, loss: 55.248506\n",
      "Epoch 68, loss: 55.248288\n",
      "Epoch 69, loss: 55.248071\n",
      "Epoch 70, loss: 55.247853\n",
      "Epoch 71, loss: 55.247635\n",
      "Epoch 72, loss: 55.247418\n",
      "Epoch 73, loss: 55.247201\n",
      "Epoch 74, loss: 55.246984\n",
      "Epoch 75, loss: 55.246767\n",
      "Epoch 76, loss: 55.246550\n",
      "Epoch 77, loss: 55.246333\n",
      "Epoch 78, loss: 55.246117\n",
      "Epoch 79, loss: 55.245901\n",
      "Epoch 80, loss: 55.245684\n",
      "Epoch 81, loss: 55.245468\n",
      "Epoch 82, loss: 55.245252\n",
      "Epoch 83, loss: 55.245037\n",
      "Epoch 84, loss: 55.244821\n",
      "Epoch 85, loss: 55.244606\n",
      "Epoch 86, loss: 55.244390\n",
      "Epoch 87, loss: 55.244175\n",
      "Epoch 88, loss: 55.243960\n",
      "Epoch 89, loss: 55.243745\n",
      "Epoch 90, loss: 55.243530\n",
      "Epoch 91, loss: 55.243316\n",
      "Epoch 92, loss: 55.243101\n",
      "Epoch 93, loss: 55.242887\n",
      "Epoch 94, loss: 55.242673\n",
      "Epoch 95, loss: 55.242459\n",
      "Epoch 96, loss: 55.242245\n",
      "Epoch 97, loss: 55.242031\n",
      "Epoch 98, loss: 55.241817\n",
      "Epoch 99, loss: 55.241604\n",
      "Epoch 100, loss: 55.241390\n",
      "Epoch 101, loss: 55.241177\n",
      "Epoch 102, loss: 55.240964\n",
      "Epoch 103, loss: 55.240751\n",
      "Epoch 104, loss: 55.240538\n",
      "Epoch 105, loss: 55.240325\n",
      "Epoch 106, loss: 55.240113\n",
      "Epoch 107, loss: 55.239900\n",
      "Epoch 108, loss: 55.239688\n",
      "Epoch 109, loss: 55.239476\n",
      "Epoch 110, loss: 55.239264\n",
      "Epoch 111, loss: 55.239052\n",
      "Epoch 112, loss: 55.238840\n",
      "Epoch 113, loss: 55.238628\n",
      "Epoch 114, loss: 55.238417\n",
      "Epoch 115, loss: 55.238205\n",
      "Epoch 116, loss: 55.237994\n",
      "Epoch 117, loss: 55.237783\n",
      "Epoch 118, loss: 55.237572\n",
      "Epoch 119, loss: 55.237361\n",
      "Epoch 120, loss: 55.237150\n",
      "Epoch 121, loss: 55.236940\n",
      "Epoch 122, loss: 55.236729\n",
      "Epoch 123, loss: 55.236519\n",
      "Epoch 124, loss: 55.236309\n",
      "Epoch 125, loss: 55.236098\n",
      "Epoch 126, loss: 55.235888\n",
      "Epoch 127, loss: 55.235679\n",
      "Epoch 128, loss: 55.235469\n",
      "Epoch 129, loss: 55.235259\n",
      "Epoch 130, loss: 55.235050\n",
      "Epoch 131, loss: 55.234840\n",
      "Epoch 132, loss: 55.234631\n",
      "Epoch 133, loss: 55.234422\n",
      "Epoch 134, loss: 55.234213\n",
      "Epoch 135, loss: 55.234004\n",
      "Epoch 136, loss: 55.233795\n",
      "Epoch 137, loss: 55.233587\n",
      "Epoch 138, loss: 55.233378\n",
      "Epoch 139, loss: 55.233170\n",
      "Epoch 140, loss: 55.232961\n",
      "Epoch 141, loss: 55.232753\n",
      "Epoch 142, loss: 55.232545\n",
      "Epoch 143, loss: 55.232337\n",
      "Epoch 144, loss: 55.232129\n",
      "Epoch 145, loss: 55.231922\n",
      "Epoch 146, loss: 55.231714\n",
      "Epoch 147, loss: 55.231507\n",
      "Epoch 148, loss: 55.231299\n",
      "Epoch 149, loss: 55.231092\n",
      "Epoch 150, loss: 55.230885\n",
      "Epoch 151, loss: 55.230678\n",
      "Epoch 152, loss: 55.230471\n",
      "Epoch 153, loss: 55.230264\n",
      "Epoch 154, loss: 55.230058\n",
      "Epoch 155, loss: 55.229851\n",
      "Epoch 156, loss: 55.229645\n",
      "Epoch 157, loss: 55.229439\n",
      "Epoch 158, loss: 55.229232\n",
      "Epoch 159, loss: 55.229026\n",
      "Epoch 160, loss: 55.228820\n",
      "Epoch 161, loss: 55.228614\n",
      "Epoch 162, loss: 55.228409\n",
      "Epoch 163, loss: 55.228203\n",
      "Epoch 164, loss: 55.227998\n",
      "Epoch 165, loss: 55.227792\n",
      "Epoch 166, loss: 55.227587\n",
      "Epoch 167, loss: 55.227382\n",
      "Epoch 168, loss: 55.227177\n",
      "Epoch 169, loss: 55.226972\n",
      "Epoch 170, loss: 55.226767\n",
      "Epoch 171, loss: 55.226562\n",
      "Epoch 172, loss: 55.226357\n",
      "Epoch 173, loss: 55.226153\n",
      "Epoch 174, loss: 55.225949\n",
      "Epoch 175, loss: 55.225744\n",
      "Epoch 176, loss: 55.225540\n",
      "Epoch 177, loss: 55.225336\n",
      "Epoch 178, loss: 55.225132\n",
      "Epoch 179, loss: 55.224928\n",
      "Epoch 180, loss: 55.224724\n",
      "Epoch 181, loss: 55.224521\n",
      "Epoch 182, loss: 55.224317\n",
      "Epoch 183, loss: 55.224114\n",
      "Epoch 184, loss: 55.223910\n",
      "Epoch 185, loss: 55.223707\n",
      "Epoch 186, loss: 55.223504\n",
      "Epoch 187, loss: 55.223301\n",
      "Epoch 188, loss: 55.223098\n",
      "Epoch 189, loss: 55.222895\n",
      "Epoch 190, loss: 55.222692\n",
      "Epoch 191, loss: 55.222490\n",
      "Epoch 192, loss: 55.222287\n",
      "Epoch 193, loss: 55.222085\n",
      "Epoch 194, loss: 55.221882\n",
      "Epoch 195, loss: 55.221680\n",
      "Epoch 196, loss: 55.221478\n",
      "Epoch 197, loss: 55.221276\n",
      "Epoch 198, loss: 55.221074\n",
      "Epoch 199, loss: 55.220872\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.259127\n",
      "Epoch 1, loss: 55.258914\n",
      "Epoch 2, loss: 55.258702\n",
      "Epoch 3, loss: 55.258490\n",
      "Epoch 4, loss: 55.258278\n",
      "Epoch 5, loss: 55.258067\n",
      "Epoch 6, loss: 55.257855\n",
      "Epoch 7, loss: 55.257644\n",
      "Epoch 8, loss: 55.257432\n",
      "Epoch 9, loss: 55.257221\n",
      "Epoch 10, loss: 55.257010\n",
      "Epoch 11, loss: 55.256799\n",
      "Epoch 12, loss: 55.256588\n",
      "Epoch 13, loss: 55.256377\n",
      "Epoch 14, loss: 55.256167\n",
      "Epoch 15, loss: 55.255956\n",
      "Epoch 16, loss: 55.255746\n",
      "Epoch 17, loss: 55.255535\n",
      "Epoch 18, loss: 55.255325\n",
      "Epoch 19, loss: 55.255115\n",
      "Epoch 20, loss: 55.254905\n",
      "Epoch 21, loss: 55.254696\n",
      "Epoch 22, loss: 55.254486\n",
      "Epoch 23, loss: 55.254276\n",
      "Epoch 24, loss: 55.254067\n",
      "Epoch 25, loss: 55.253858\n",
      "Epoch 26, loss: 55.253648\n",
      "Epoch 27, loss: 55.253439\n",
      "Epoch 28, loss: 55.253230\n",
      "Epoch 29, loss: 55.253021\n",
      "Epoch 30, loss: 55.252813\n",
      "Epoch 31, loss: 55.252604\n",
      "Epoch 32, loss: 55.252396\n",
      "Epoch 33, loss: 55.252187\n",
      "Epoch 34, loss: 55.251979\n",
      "Epoch 35, loss: 55.251771\n",
      "Epoch 36, loss: 55.251563\n",
      "Epoch 37, loss: 55.251355\n",
      "Epoch 38, loss: 55.251147\n",
      "Epoch 39, loss: 55.250939\n",
      "Epoch 40, loss: 55.250731\n",
      "Epoch 41, loss: 55.250524\n",
      "Epoch 42, loss: 55.250316\n",
      "Epoch 43, loss: 55.250109\n",
      "Epoch 44, loss: 55.249902\n",
      "Epoch 45, loss: 55.249695\n",
      "Epoch 46, loss: 55.249488\n",
      "Epoch 47, loss: 55.249281\n",
      "Epoch 48, loss: 55.249074\n",
      "Epoch 49, loss: 55.248868\n",
      "Epoch 50, loss: 55.248661\n",
      "Epoch 51, loss: 55.248455\n",
      "Epoch 52, loss: 55.248249\n",
      "Epoch 53, loss: 55.248042\n",
      "Epoch 54, loss: 55.247836\n",
      "Epoch 55, loss: 55.247630\n",
      "Epoch 56, loss: 55.247424\n",
      "Epoch 57, loss: 55.247219\n",
      "Epoch 58, loss: 55.247013\n",
      "Epoch 59, loss: 55.246807\n",
      "Epoch 60, loss: 55.246602\n",
      "Epoch 61, loss: 55.246396\n",
      "Epoch 62, loss: 55.246191\n",
      "Epoch 63, loss: 55.245986\n",
      "Epoch 64, loss: 55.245781\n",
      "Epoch 65, loss: 55.245576\n",
      "Epoch 66, loss: 55.245371\n",
      "Epoch 67, loss: 55.245167\n",
      "Epoch 68, loss: 55.244962\n",
      "Epoch 69, loss: 55.244757\n",
      "Epoch 70, loss: 55.244553\n",
      "Epoch 71, loss: 55.244349\n",
      "Epoch 72, loss: 55.244144\n",
      "Epoch 73, loss: 55.243940\n",
      "Epoch 74, loss: 55.243736\n",
      "Epoch 75, loss: 55.243532\n",
      "Epoch 76, loss: 55.243328\n",
      "Epoch 77, loss: 55.243125\n",
      "Epoch 78, loss: 55.242921\n",
      "Epoch 79, loss: 55.242718\n",
      "Epoch 80, loss: 55.242514\n",
      "Epoch 81, loss: 55.242311\n",
      "Epoch 82, loss: 55.242108\n",
      "Epoch 83, loss: 55.241904\n",
      "Epoch 84, loss: 55.241701\n",
      "Epoch 85, loss: 55.241498\n",
      "Epoch 86, loss: 55.241296\n",
      "Epoch 87, loss: 55.241093\n",
      "Epoch 88, loss: 55.240890\n",
      "Epoch 89, loss: 55.240688\n",
      "Epoch 90, loss: 55.240485\n",
      "Epoch 91, loss: 55.240283\n",
      "Epoch 92, loss: 55.240080\n",
      "Epoch 93, loss: 55.239878\n",
      "Epoch 94, loss: 55.239676\n",
      "Epoch 95, loss: 55.239474\n",
      "Epoch 96, loss: 55.239272\n",
      "Epoch 97, loss: 55.239071\n",
      "Epoch 98, loss: 55.238869\n",
      "Epoch 99, loss: 55.238667\n",
      "Epoch 100, loss: 55.238466\n",
      "Epoch 101, loss: 55.238264\n",
      "Epoch 102, loss: 55.238063\n",
      "Epoch 103, loss: 55.237862\n",
      "Epoch 104, loss: 55.237661\n",
      "Epoch 105, loss: 55.237459\n",
      "Epoch 106, loss: 55.237258\n",
      "Epoch 107, loss: 55.237058\n",
      "Epoch 108, loss: 55.236857\n",
      "Epoch 109, loss: 55.236656\n",
      "Epoch 110, loss: 55.236455\n",
      "Epoch 111, loss: 55.236255\n",
      "Epoch 112, loss: 55.236055\n",
      "Epoch 113, loss: 55.235854\n",
      "Epoch 114, loss: 55.235654\n",
      "Epoch 115, loss: 55.235454\n",
      "Epoch 116, loss: 55.235254\n",
      "Epoch 117, loss: 55.235054\n",
      "Epoch 118, loss: 55.234854\n",
      "Epoch 119, loss: 55.234654\n",
      "Epoch 120, loss: 55.234454\n",
      "Epoch 121, loss: 55.234255\n",
      "Epoch 122, loss: 55.234055\n",
      "Epoch 123, loss: 55.233856\n",
      "Epoch 124, loss: 55.233656\n",
      "Epoch 125, loss: 55.233457\n",
      "Epoch 126, loss: 55.233258\n",
      "Epoch 127, loss: 55.233059\n",
      "Epoch 128, loss: 55.232860\n",
      "Epoch 129, loss: 55.232661\n",
      "Epoch 130, loss: 55.232462\n",
      "Epoch 131, loss: 55.232263\n",
      "Epoch 132, loss: 55.232064\n",
      "Epoch 133, loss: 55.231866\n",
      "Epoch 134, loss: 55.231667\n",
      "Epoch 135, loss: 55.231469\n",
      "Epoch 136, loss: 55.231270\n",
      "Epoch 137, loss: 55.231072\n",
      "Epoch 138, loss: 55.230874\n",
      "Epoch 139, loss: 55.230676\n",
      "Epoch 140, loss: 55.230478\n",
      "Epoch 141, loss: 55.230280\n",
      "Epoch 142, loss: 55.230082\n",
      "Epoch 143, loss: 55.229884\n",
      "Epoch 144, loss: 55.229687\n",
      "Epoch 145, loss: 55.229489\n",
      "Epoch 146, loss: 55.229291\n",
      "Epoch 147, loss: 55.229094\n",
      "Epoch 148, loss: 55.228897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149, loss: 55.228699\n",
      "Epoch 150, loss: 55.228502\n",
      "Epoch 151, loss: 55.228305\n",
      "Epoch 152, loss: 55.228108\n",
      "Epoch 153, loss: 55.227911\n",
      "Epoch 154, loss: 55.227714\n",
      "Epoch 155, loss: 55.227517\n",
      "Epoch 156, loss: 55.227321\n",
      "Epoch 157, loss: 55.227124\n",
      "Epoch 158, loss: 55.226927\n",
      "Epoch 159, loss: 55.226731\n",
      "Epoch 160, loss: 55.226534\n",
      "Epoch 161, loss: 55.226338\n",
      "Epoch 162, loss: 55.226142\n",
      "Epoch 163, loss: 55.225946\n",
      "Epoch 164, loss: 55.225750\n",
      "Epoch 165, loss: 55.225553\n",
      "Epoch 166, loss: 55.225358\n",
      "Epoch 167, loss: 55.225162\n",
      "Epoch 168, loss: 55.224966\n",
      "Epoch 169, loss: 55.224770\n",
      "Epoch 170, loss: 55.224574\n",
      "Epoch 171, loss: 55.224379\n",
      "Epoch 172, loss: 55.224183\n",
      "Epoch 173, loss: 55.223988\n",
      "Epoch 174, loss: 55.223793\n",
      "Epoch 175, loss: 55.223597\n",
      "Epoch 176, loss: 55.223402\n",
      "Epoch 177, loss: 55.223207\n",
      "Epoch 178, loss: 55.223012\n",
      "Epoch 179, loss: 55.222817\n",
      "Epoch 180, loss: 55.222622\n",
      "Epoch 181, loss: 55.222427\n",
      "Epoch 182, loss: 55.222232\n",
      "Epoch 183, loss: 55.222038\n",
      "Epoch 184, loss: 55.221843\n",
      "Epoch 185, loss: 55.221649\n",
      "Epoch 186, loss: 55.221454\n",
      "Epoch 187, loss: 55.221260\n",
      "Epoch 188, loss: 55.221065\n",
      "Epoch 189, loss: 55.220871\n",
      "Epoch 190, loss: 55.220677\n",
      "Epoch 191, loss: 55.220483\n",
      "Epoch 192, loss: 55.220289\n",
      "Epoch 193, loss: 55.220095\n",
      "Epoch 194, loss: 55.219901\n",
      "Epoch 195, loss: 55.219707\n",
      "Epoch 196, loss: 55.219513\n",
      "Epoch 197, loss: 55.219320\n",
      "Epoch 198, loss: 55.219126\n",
      "Epoch 199, loss: 55.218933\n",
      "(3073, 10)\n",
      "Epoch 0, loss: 55.258442\n",
      "Epoch 1, loss: 55.258219\n",
      "Epoch 2, loss: 55.257996\n",
      "Epoch 3, loss: 55.257774\n",
      "Epoch 4, loss: 55.257552\n",
      "Epoch 5, loss: 55.257329\n",
      "Epoch 6, loss: 55.257108\n",
      "Epoch 7, loss: 55.256886\n",
      "Epoch 8, loss: 55.256664\n",
      "Epoch 9, loss: 55.256443\n",
      "Epoch 10, loss: 55.256221\n",
      "Epoch 11, loss: 55.256000\n",
      "Epoch 12, loss: 55.255779\n",
      "Epoch 13, loss: 55.255559\n",
      "Epoch 14, loss: 55.255338\n",
      "Epoch 15, loss: 55.255117\n",
      "Epoch 16, loss: 55.254897\n",
      "Epoch 17, loss: 55.254677\n",
      "Epoch 18, loss: 55.254457\n",
      "Epoch 19, loss: 55.254237\n",
      "Epoch 20, loss: 55.254017\n",
      "Epoch 21, loss: 55.253798\n",
      "Epoch 22, loss: 55.253578\n",
      "Epoch 23, loss: 55.253359\n",
      "Epoch 24, loss: 55.253140\n",
      "Epoch 25, loss: 55.252921\n",
      "Epoch 26, loss: 55.252702\n",
      "Epoch 27, loss: 55.252484\n",
      "Epoch 28, loss: 55.252265\n",
      "Epoch 29, loss: 55.252047\n",
      "Epoch 30, loss: 55.251829\n",
      "Epoch 31, loss: 55.251611\n",
      "Epoch 32, loss: 55.251393\n",
      "Epoch 33, loss: 55.251175\n",
      "Epoch 34, loss: 55.250957\n",
      "Epoch 35, loss: 55.250740\n",
      "Epoch 36, loss: 55.250523\n",
      "Epoch 37, loss: 55.250306\n",
      "Epoch 38, loss: 55.250089\n",
      "Epoch 39, loss: 55.249872\n",
      "Epoch 40, loss: 55.249655\n",
      "Epoch 41, loss: 55.249438\n",
      "Epoch 42, loss: 55.249222\n",
      "Epoch 43, loss: 55.249006\n",
      "Epoch 44, loss: 55.248790\n",
      "Epoch 45, loss: 55.248574\n",
      "Epoch 46, loss: 55.248358\n",
      "Epoch 47, loss: 55.248142\n",
      "Epoch 48, loss: 55.247927\n",
      "Epoch 49, loss: 55.247711\n",
      "Epoch 50, loss: 55.247496\n",
      "Epoch 51, loss: 55.247281\n",
      "Epoch 52, loss: 55.247066\n",
      "Epoch 53, loss: 55.246851\n",
      "Epoch 54, loss: 55.246636\n",
      "Epoch 55, loss: 55.246422\n",
      "Epoch 56, loss: 55.246207\n",
      "Epoch 57, loss: 55.245993\n",
      "Epoch 58, loss: 55.245779\n",
      "Epoch 59, loss: 55.245565\n",
      "Epoch 60, loss: 55.245351\n",
      "Epoch 61, loss: 55.245137\n",
      "Epoch 62, loss: 55.244923\n",
      "Epoch 63, loss: 55.244710\n",
      "Epoch 64, loss: 55.244497\n",
      "Epoch 65, loss: 55.244283\n",
      "Epoch 66, loss: 55.244070\n",
      "Epoch 67, loss: 55.243857\n",
      "Epoch 68, loss: 55.243645\n",
      "Epoch 69, loss: 55.243432\n",
      "Epoch 70, loss: 55.243219\n",
      "Epoch 71, loss: 55.243007\n",
      "Epoch 72, loss: 55.242795\n",
      "Epoch 73, loss: 55.242583\n",
      "Epoch 74, loss: 55.242371\n",
      "Epoch 75, loss: 55.242159\n",
      "Epoch 76, loss: 55.241947\n",
      "Epoch 77, loss: 55.241735\n",
      "Epoch 78, loss: 55.241524\n",
      "Epoch 79, loss: 55.241312\n",
      "Epoch 80, loss: 55.241101\n",
      "Epoch 81, loss: 55.240890\n",
      "Epoch 82, loss: 55.240679\n",
      "Epoch 83, loss: 55.240468\n",
      "Epoch 84, loss: 55.240258\n",
      "Epoch 85, loss: 55.240047\n",
      "Epoch 86, loss: 55.239836\n",
      "Epoch 87, loss: 55.239626\n",
      "Epoch 88, loss: 55.239416\n",
      "Epoch 89, loss: 55.239206\n",
      "Epoch 90, loss: 55.238996\n",
      "Epoch 91, loss: 55.238786\n",
      "Epoch 92, loss: 55.238576\n",
      "Epoch 93, loss: 55.238367\n",
      "Epoch 94, loss: 55.238157\n",
      "Epoch 95, loss: 55.237948\n",
      "Epoch 96, loss: 55.237739\n",
      "Epoch 97, loss: 55.237529\n",
      "Epoch 98, loss: 55.237320\n",
      "Epoch 99, loss: 55.237112\n",
      "Epoch 100, loss: 55.236903\n",
      "Epoch 101, loss: 55.236694\n",
      "Epoch 102, loss: 55.236486\n",
      "Epoch 103, loss: 55.236277\n",
      "Epoch 104, loss: 55.236069\n",
      "Epoch 105, loss: 55.235861\n",
      "Epoch 106, loss: 55.235653\n",
      "Epoch 107, loss: 55.235445\n",
      "Epoch 108, loss: 55.235237\n",
      "Epoch 109, loss: 55.235030\n",
      "Epoch 110, loss: 55.234822\n",
      "Epoch 111, loss: 55.234614\n",
      "Epoch 112, loss: 55.234407\n",
      "Epoch 113, loss: 55.234200\n",
      "Epoch 114, loss: 55.233993\n",
      "Epoch 115, loss: 55.233786\n",
      "Epoch 116, loss: 55.233579\n",
      "Epoch 117, loss: 55.233372\n",
      "Epoch 118, loss: 55.233166\n",
      "Epoch 119, loss: 55.232959\n",
      "Epoch 120, loss: 55.232753\n",
      "Epoch 121, loss: 55.232546\n",
      "Epoch 122, loss: 55.232340\n",
      "Epoch 123, loss: 55.232134\n",
      "Epoch 124, loss: 55.231928\n",
      "Epoch 125, loss: 55.231722\n",
      "Epoch 126, loss: 55.231517\n",
      "Epoch 127, loss: 55.231311\n",
      "Epoch 128, loss: 55.231105\n",
      "Epoch 129, loss: 55.230900\n",
      "Epoch 130, loss: 55.230695\n",
      "Epoch 131, loss: 55.230490\n",
      "Epoch 132, loss: 55.230285\n",
      "Epoch 133, loss: 55.230080\n",
      "Epoch 134, loss: 55.229875\n",
      "Epoch 135, loss: 55.229670\n",
      "Epoch 136, loss: 55.229465\n",
      "Epoch 137, loss: 55.229261\n",
      "Epoch 138, loss: 55.229056\n",
      "Epoch 139, loss: 55.228852\n",
      "Epoch 140, loss: 55.228648\n",
      "Epoch 141, loss: 55.228444\n",
      "Epoch 142, loss: 55.228240\n",
      "Epoch 143, loss: 55.228036\n",
      "Epoch 144, loss: 55.227832\n",
      "Epoch 145, loss: 55.227628\n",
      "Epoch 146, loss: 55.227425\n",
      "Epoch 147, loss: 55.227221\n",
      "Epoch 148, loss: 55.227018\n",
      "Epoch 149, loss: 55.226815\n",
      "Epoch 150, loss: 55.226612\n",
      "Epoch 151, loss: 55.226409\n",
      "Epoch 152, loss: 55.226206\n",
      "Epoch 153, loss: 55.226003\n",
      "Epoch 154, loss: 55.225800\n",
      "Epoch 155, loss: 55.225597\n",
      "Epoch 156, loss: 55.225395\n",
      "Epoch 157, loss: 55.225192\n",
      "Epoch 158, loss: 55.224990\n",
      "Epoch 159, loss: 55.224788\n",
      "Epoch 160, loss: 55.224586\n",
      "Epoch 161, loss: 55.224384\n",
      "Epoch 162, loss: 55.224182\n",
      "Epoch 163, loss: 55.223980\n",
      "Epoch 164, loss: 55.223778\n",
      "Epoch 165, loss: 55.223576\n",
      "Epoch 166, loss: 55.223375\n",
      "Epoch 167, loss: 55.223174\n",
      "Epoch 168, loss: 55.222972\n",
      "Epoch 169, loss: 55.222771\n",
      "Epoch 170, loss: 55.222570\n",
      "Epoch 171, loss: 55.222369\n",
      "Epoch 172, loss: 55.222168\n",
      "Epoch 173, loss: 55.221967\n",
      "Epoch 174, loss: 55.221766\n",
      "Epoch 175, loss: 55.221565\n",
      "Epoch 176, loss: 55.221365\n",
      "Epoch 177, loss: 55.221164\n",
      "Epoch 178, loss: 55.220964\n",
      "Epoch 179, loss: 55.220764\n",
      "Epoch 180, loss: 55.220564\n",
      "Epoch 181, loss: 55.220364\n",
      "Epoch 182, loss: 55.220164\n",
      "Epoch 183, loss: 55.219964\n",
      "Epoch 184, loss: 55.219764\n",
      "Epoch 185, loss: 55.219564\n",
      "Epoch 186, loss: 55.219364\n",
      "Epoch 187, loss: 55.219165\n",
      "Epoch 188, loss: 55.218965\n",
      "Epoch 189, loss: 55.218766\n",
      "Epoch 190, loss: 55.218567\n",
      "Epoch 191, loss: 55.218368\n",
      "Epoch 192, loss: 55.218169\n",
      "Epoch 193, loss: 55.217970\n",
      "Epoch 194, loss: 55.217771\n",
      "Epoch 195, loss: 55.217572\n",
      "Epoch 196, loss: 55.217373\n",
      "Epoch 197, loss: 55.217175\n",
      "Epoch 198, loss: 55.216976\n",
      "Epoch 199, loss: 55.216778\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         classifiers\u001b[38;5;241m.\u001b[39mappend(classifier) \n\u001b[0;32m     25\u001b[0m         val_accuracy\u001b[38;5;241m.\u001b[39mappend(multiclass_accuracy(classifier\u001b[38;5;241m.\u001b[39mpredict(houldounX_dataset), houldounY_dataset))\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest validation accuracy achieved: \u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_val_accuracy\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "classifiers = []\n",
    "val_accuracy = []\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "fitX_dataset = train_X[:int(train_X.shape[0] * 0.8)]\n",
    "houldounX_dataset = train_X[:int(train_X.shape[0] * 0.2)]\n",
    "\n",
    "fitY_dataset = train_y[:int(train_X.shape[0] * 0.8)]\n",
    "houldounY_dataset = train_y[:int(train_X.shape[0] * 0.2)]\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(fitX_dataset,fitY_dataset,batch_size, i, j ,num_epochs)\n",
    "        classifiers.append(classifier) \n",
    "        val_accuracy.append(multiclass_accuracy(classifier.predict(houldounX_dataset), houldounY_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.232778\n"
     ]
    }
   ],
   "source": [
    "best_classifier = classifiers[0]\n",
    "best_val_accuracy = val_accuracy[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
